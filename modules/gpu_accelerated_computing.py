#!/usr/bin/env python3
"""
GPUåŠ é€Ÿè®¡ç®—æ¨¡å—
æä¾›CUDAåŠ é€Ÿçš„å¹¶è¡Œè®¡ç®—åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import cupy as cp
from numba import cuda, jit
from typing import Dict, List, Optional, Tuple, Union
import logging
import time
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)


class GPUAcceleratedComputing:
    """GPUåŠ é€Ÿè®¡ç®—ç®¡ç†å™¨"""
    
    def __init__(self):
        self.device = self._detect_best_device()
        self.cuda_available = torch.cuda.is_available()
        self.cupy_available = self._check_cupy()
        self.memory_pool = None
        
        if self.cuda_available:
            self._initialize_gpu()
        
        print(f"[GPUåŠ é€Ÿ] åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   CUDAå¯ç”¨: {self.cuda_available}")
        print(f"   CuPyå¯ç”¨: {self.cupy_available}")
    
    def _detect_best_device(self) -> str:
        """æ£€æµ‹æœ€ä½³è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            # æ£€æµ‹GPUå‹å·å’Œæ€§èƒ½
            gpu_count = torch.cuda.device_count()
            best_device = 'cuda:0'
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory
                
                print(f"[GPU {i}] {gpu_name}, å†…å­˜: {gpu_memory // 1024**3}GB")
                
                # RTX 50ç³»åˆ—ä¼˜å…ˆ
                if 'RTX 50' in gpu_name or 'RTX 5' in gpu_name:
                    best_device = f'cuda:{i}'
                    break
            
            return best_device
        else:
            return 'cpu'
    
    def _check_cupy(self) -> bool:
        """æ£€æŸ¥CuPyå¯ç”¨æ€§"""
        try:
            import cupy
            return True
        except ImportError:
            return False
    
    def _initialize_gpu(self):
        """åˆå§‹åŒ–GPUè®¾ç½®"""
        try:
            # è®¾ç½®GPUå†…å­˜ç®¡ç†
            torch.cuda.empty_cache()
            
            # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            if hasattr(torch.cuda, 'amp'):
                self.use_amp = True
            else:
                self.use_amp = False
            
            # åˆå§‹åŒ–CuPyå†…å­˜æ± 
            if self.cupy_available:
                self.memory_pool = cp.get_default_memory_pool()
                self.memory_pool.set_limit(size=2**30)  # 1GBé™åˆ¶
            
            print(f"[GPU] åˆå§‹åŒ–æˆåŠŸï¼Œæ··åˆç²¾åº¦: {self.use_amp}")
            
        except Exception as e:
            logger.error(f"GPUåˆå§‹åŒ–å¤±è´¥: {e}")
    
    def accelerated_technical_analysis(self, prices: np.ndarray, volumes: np.ndarray = None) -> Dict:
        """GPUåŠ é€ŸæŠ€æœ¯åˆ†æè®¡ç®—"""
        try:
            if self.cupy_available and len(prices) > 1000:
                return self._cupy_technical_analysis(prices, volumes)
            else:
                return self._torch_technical_analysis(prices, volumes)
                
        except Exception as e:
            logger.error(f"GPUæŠ€æœ¯åˆ†æå¤±è´¥: {e}")
            return self._cpu_fallback_analysis(prices, volumes)
    
    def _cupy_technical_analysis(self, prices: np.ndarray, volumes: np.ndarray = None) -> Dict:
        """ä½¿ç”¨CuPyè¿›è¡ŒæŠ€æœ¯åˆ†æ"""
        try:
            # è½¬æ¢ä¸ºCuPyæ•°ç»„
            prices_gpu = cp.asarray(prices)
            
            results = {}
            
            # ç§»åŠ¨å¹³å‡çº¿
            results['sma_5'] = self._gpu_sma(prices_gpu, 5)
            results['sma_20'] = self._gpu_sma(prices_gpu, 20)
            results['sma_50'] = self._gpu_sma(prices_gpu, 50)
            
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            results['ema_12'] = self._gpu_ema(prices_gpu, 12)
            results['ema_26'] = self._gpu_ema(prices_gpu, 26)
            
            # MACD
            results['macd'] = self._gpu_macd(prices_gpu)
            
            # RSI
            results['rsi'] = self._gpu_rsi(prices_gpu)
            
            # å¸ƒæ—å¸¦
            results['bollinger'] = self._gpu_bollinger_bands(prices_gpu)
            
            # æ³¢åŠ¨ç‡
            results['volatility'] = self._gpu_volatility(prices_gpu)
            
            # å¦‚æœæœ‰æˆäº¤é‡æ•°æ®
            if volumes is not None:
                volumes_gpu = cp.asarray(volumes)
                results['volume_sma'] = self._gpu_sma(volumes_gpu, 20)
                results['obv'] = self._gpu_obv(prices_gpu, volumes_gpu)
            
            # è½¬æ¢å›CPU
            for key, value in results.items():
                if isinstance(value, dict):
                    for sub_key, sub_value in value.items():
                        if hasattr(sub_value, 'get'):
                            results[key][sub_key] = float(cp.asnumpy(sub_value))
                else:
                    if hasattr(value, 'get'):
                        results[key] = float(cp.asnumpy(value))
            
            return results
            
        except Exception as e:
            logger.error(f"CuPyæŠ€æœ¯åˆ†æå¤±è´¥: {e}")
            return self._torch_technical_analysis(prices, volumes)
    
    def _gpu_sma(self, prices: cp.ndarray, period: int) -> cp.ndarray:
        """GPUåŠ é€Ÿç®€å•ç§»åŠ¨å¹³å‡"""
        if len(prices) < period:
            return prices[-1] if len(prices) > 0 else cp.array([0])
        
        # ä½¿ç”¨å·ç§¯è®¡ç®—ç§»åŠ¨å¹³å‡
        kernel = cp.ones(period) / period
        sma = cp.convolve(prices, kernel, mode='valid')
        return sma[-1] if len(sma) > 0 else prices[-1]
    
    def _gpu_ema(self, prices: cp.ndarray, period: int) -> cp.ndarray:
        """GPUåŠ é€ŸæŒ‡æ•°ç§»åŠ¨å¹³å‡"""
        if len(prices) < 2:
            return prices[-1] if len(prices) > 0 else cp.array([0])
        
        alpha = 2.0 / (period + 1)
        ema = cp.zeros_like(prices)
        ema[0] = prices[0]
        
        for i in range(1, len(prices)):
            ema[i] = alpha * prices[i] + (1 - alpha) * ema[i-1]
        
        return ema[-1]
    
    def _gpu_macd(self, prices: cp.ndarray) -> Dict:
        """GPUåŠ é€ŸMACDè®¡ç®—"""
        if len(prices) < 26:
            return {'macd': 0, 'signal': 0, 'histogram': 0}
        
        ema_12 = self._gpu_ema(prices, 12)
        ema_26 = self._gpu_ema(prices, 26)
        macd_line = ema_12 - ema_26
        
        # ä¿¡å·çº¿ï¼ˆMACDçš„9æ—¥EMAï¼‰
        macd_array = cp.array([macd_line])
        signal_line = self._gpu_ema(macd_array, 9)
        histogram = macd_line - signal_line
        
        return {
            'macd': float(cp.asnumpy(macd_line)),
            'signal': float(cp.asnumpy(signal_line)),
            'histogram': float(cp.asnumpy(histogram))
        }
    
    def _gpu_rsi(self, prices: cp.ndarray, period: int = 14) -> cp.ndarray:
        """GPUåŠ é€ŸRSIè®¡ç®—"""
        if len(prices) < period + 1:
            return cp.array([50])
        
        # è®¡ç®—ä»·æ ¼å˜åŒ–
        deltas = cp.diff(prices)
        gains = cp.where(deltas > 0, deltas, 0)
        losses = cp.where(deltas < 0, -deltas, 0)
        
        # è®¡ç®—å¹³å‡æ”¶ç›Šå’ŒæŸå¤±
        avg_gain = cp.mean(gains[-period:])
        avg_loss = cp.mean(losses[-period:])
        
        if avg_loss == 0:
            return cp.array([100])
        
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        
        return rsi
    
    def _gpu_bollinger_bands(self, prices: cp.ndarray, period: int = 20, std_dev: float = 2) -> Dict:
        """GPUåŠ é€Ÿå¸ƒæ—å¸¦è®¡ç®—"""
        if len(prices) < period:
            current_price = prices[-1] if len(prices) > 0 else 0
            return {
                'upper': float(cp.asnumpy(current_price * 1.01)),
                'middle': float(cp.asnumpy(current_price)),
                'lower': float(cp.asnumpy(current_price * 0.99))
            }
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡å’Œæ ‡å‡†å·®
        sma = self._gpu_sma(prices, period)
        recent_prices = prices[-period:]
        std = cp.std(recent_prices)
        
        upper = sma + (std_dev * std)
        lower = sma - (std_dev * std)
        
        return {
            'upper': float(cp.asnumpy(upper)),
            'middle': float(cp.asnumpy(sma)),
            'lower': float(cp.asnumpy(lower))
        }
    
    def _gpu_volatility(self, prices: cp.ndarray, period: int = 20) -> cp.ndarray:
        """GPUåŠ é€Ÿæ³¢åŠ¨ç‡è®¡ç®—"""
        if len(prices) < 2:
            return cp.array([0])
        
        # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
        log_returns = cp.diff(cp.log(prices))
        
        # è®¡ç®—æ»šåŠ¨æ ‡å‡†å·®
        if len(log_returns) >= period:
            volatility = cp.std(log_returns[-period:]) * cp.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
        else:
            volatility = cp.std(log_returns) * cp.sqrt(252)
        
        return volatility
    
    def _gpu_obv(self, prices: cp.ndarray, volumes: cp.ndarray) -> cp.ndarray:
        """GPUåŠ é€ŸOBVè®¡ç®—"""
        if len(prices) < 2 or len(volumes) < 2:
            return cp.array([0])
        
        price_changes = cp.diff(prices)
        obv = cp.zeros(len(volumes))
        
        for i in range(1, len(volumes)):
            if price_changes[i-1] > 0:
                obv[i] = obv[i-1] + volumes[i]
            elif price_changes[i-1] < 0:
                obv[i] = obv[i-1] - volumes[i]
            else:
                obv[i] = obv[i-1]
        
        return obv[-1]
    
    def _torch_technical_analysis(self, prices: np.ndarray, volumes: np.ndarray = None) -> Dict:
        """ä½¿ç”¨PyTorchè¿›è¡ŒæŠ€æœ¯åˆ†æ"""
        try:
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            prices_tensor = torch.FloatTensor(prices).to(self.device)
            
            results = {}
            
            # ç§»åŠ¨å¹³å‡çº¿
            results['sma_5'] = self._torch_sma(prices_tensor, 5)
            results['sma_20'] = self._torch_sma(prices_tensor, 20)
            
            # ç®€åŒ–çš„æŠ€æœ¯æŒ‡æ ‡
            if len(prices) >= 2:
                returns = torch.diff(prices_tensor)
                results['volatility'] = float(torch.std(returns).cpu())
                results['momentum'] = float((prices_tensor[-1] - prices_tensor[-min(10, len(prices_tensor))]).cpu())
            
            # è½¬æ¢å›CPU
            for key, value in results.items():
                if torch.is_tensor(value):
                    results[key] = float(value.cpu())
            
            return results
            
        except Exception as e:
            logger.error(f"PyTorchæŠ€æœ¯åˆ†æå¤±è´¥: {e}")
            return self._cpu_fallback_analysis(prices, volumes)
    
    def _torch_sma(self, prices: torch.Tensor, period: int) -> torch.Tensor:
        """PyTorchç®€å•ç§»åŠ¨å¹³å‡"""
        if len(prices) < period:
            return prices[-1] if len(prices) > 0 else torch.tensor(0.0)
        
        return torch.mean(prices[-period:])
    
    def _cpu_fallback_analysis(self, prices: np.ndarray, volumes: np.ndarray = None) -> Dict:
        """CPUå¤‡ç”¨æŠ€æœ¯åˆ†æ"""
        results = {}
        
        if len(prices) >= 5:
            results['sma_5'] = np.mean(prices[-5:])
        if len(prices) >= 20:
            results['sma_20'] = np.mean(prices[-20:])
        
        if len(prices) >= 2:
            returns = np.diff(prices)
            results['volatility'] = np.std(returns)
            results['momentum'] = prices[-1] - prices[-min(10, len(prices))]
        
        return results
    
    def batch_prediction(self, models: List[nn.Module], data_batches: List[torch.Tensor]) -> List[torch.Tensor]:
        """æ‰¹é‡é¢„æµ‹å¤„ç†"""
        try:
            if not self.cuda_available:
                return self._cpu_batch_prediction(models, data_batches)
            
            results = []
            
            # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    for model, batch in zip(models, data_batches):
                        model.eval()
                        with torch.no_grad():
                            batch_gpu = batch.to(self.device)
                            prediction = model(batch_gpu)
                            results.append(prediction.cpu())
            else:
                for model, batch in zip(models, data_batches):
                    model.eval()
                    with torch.no_grad():
                        batch_gpu = batch.to(self.device)
                        prediction = model(batch_gpu)
                        results.append(prediction.cpu())
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            return self._cpu_batch_prediction(models, data_batches)
    
    def _cpu_batch_prediction(self, models: List[nn.Module], data_batches: List[torch.Tensor]) -> List[torch.Tensor]:
        """CPUæ‰¹é‡é¢„æµ‹"""
        results = []
        
        for model, batch in zip(models, data_batches):
            model.eval()
            with torch.no_grad():
                prediction = model(batch)
                results.append(prediction)
        
        return results
    
    def parallel_feature_extraction(self, data: np.ndarray, feature_functions: List) -> Dict:
        """å¹¶è¡Œç‰¹å¾æå–"""
        try:
            if self.cupy_available and len(data) > 1000:
                return self._gpu_feature_extraction(data, feature_functions)
            else:
                return self._cpu_feature_extraction(data, feature_functions)
                
        except Exception as e:
            logger.error(f"å¹¶è¡Œç‰¹å¾æå–å¤±è´¥: {e}")
            return self._cpu_feature_extraction(data, feature_functions)
    
    def _gpu_feature_extraction(self, data: np.ndarray, feature_functions: List) -> Dict:
        """GPUå¹¶è¡Œç‰¹å¾æå–"""
        data_gpu = cp.asarray(data)
        features = {}
        
        # å¹¶è¡Œè®¡ç®—å¤šä¸ªç‰¹å¾
        for i, func in enumerate(feature_functions):
            try:
                feature_name = f"feature_{i}"
                features[feature_name] = cp.asnumpy(func(data_gpu))
            except Exception as e:
                logger.error(f"ç‰¹å¾ {i} è®¡ç®—å¤±è´¥: {e}")
                features[f"feature_{i}"] = 0
        
        return features
    
    def _cpu_feature_extraction(self, data: np.ndarray, feature_functions: List) -> Dict:
        """CPUç‰¹å¾æå–"""
        features = {}
        
        for i, func in enumerate(feature_functions):
            try:
                feature_name = f"feature_{i}"
                features[feature_name] = func(data)
            except Exception as e:
                logger.error(f"ç‰¹å¾ {i} è®¡ç®—å¤±è´¥: {e}")
                features[f"feature_{i}"] = 0
        
        return features
    
    def optimize_memory_usage(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        try:
            if self.cuda_available:
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            if self.cupy_available and self.memory_pool:
                self.memory_pool.free_all_blocks()
            
            print("[GPU] å†…å­˜ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")
    
    def get_gpu_info(self) -> Dict:
        """è·å–GPUä¿¡æ¯"""
        info = {
            'cuda_available': self.cuda_available,
            'cupy_available': self.cupy_available,
            'device': self.device,
            'use_amp': getattr(self, 'use_amp', False)
        }
        
        if self.cuda_available:
            info['gpu_count'] = torch.cuda.device_count()
            info['current_device'] = torch.cuda.current_device()
            
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                info[f'gpu_{i}'] = {
                    'name': props.name,
                    'total_memory': props.total_memory // 1024**3,  # GB
                    'major': props.major,
                    'minor': props.minor
                }
        
        return info
    
    def benchmark_performance(self, data_size: int = 10000) -> Dict:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"[åŸºå‡†æµ‹è¯•] å¼€å§‹æ€§èƒ½æµ‹è¯•ï¼Œæ•°æ®å¤§å°: {data_size}")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = np.random.randn(data_size).astype(np.float32)
        
        results = {}
        
        # CPUåŸºå‡†æµ‹è¯•
        start_time = time.time()
        cpu_result = self._cpu_fallback_analysis(test_data)
        cpu_time = time.time() - start_time
        results['cpu_time'] = cpu_time
        
        # GPUåŸºå‡†æµ‹è¯•
        if self.cuda_available:
            start_time = time.time()
            gpu_result = self.accelerated_technical_analysis(test_data)
            gpu_time = time.time() - start_time
            results['gpu_time'] = gpu_time
            results['speedup'] = cpu_time / gpu_time if gpu_time > 0 else 1.0
        
        print(f"[åŸºå‡†æµ‹è¯•] å®Œæˆ")
        print(f"   CPUæ—¶é—´: {results['cpu_time']:.4f}s")
        if 'gpu_time' in results:
            print(f"   GPUæ—¶é—´: {results['gpu_time']:.4f}s")
            print(f"   åŠ é€Ÿæ¯”: {results['speedup']:.2f}x")
        
        return results


def main():
    """æµ‹è¯•å‡½æ•°"""
    print("GPUåŠ é€Ÿè®¡ç®—æ¨¡å—æµ‹è¯•")
    print("=" * 40)
    
    # åˆ›å»ºGPUè®¡ç®—ç®¡ç†å™¨
    gpu_compute = GPUAcceleratedComputing()
    
    try:
        # æ˜¾ç¤ºGPUä¿¡æ¯
        info = gpu_compute.get_gpu_info()
        print(f"âœ… GPUä¿¡æ¯:")
        for key, value in info.items():
            print(f"   {key}: {value}")
        
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        print(f"\nğŸš€ æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        benchmark = gpu_compute.benchmark_performance(data_size=50000)
        
        # æµ‹è¯•æŠ€æœ¯åˆ†æåŠ é€Ÿ
        print(f"\nğŸ“Š æµ‹è¯•GPUåŠ é€ŸæŠ€æœ¯åˆ†æ...")
        test_prices = np.random.randn(10000) * 10 + 3400
        test_volumes = np.random.randint(1000, 5000, 10000)
        
        start_time = time.time()
        analysis_result = gpu_compute.accelerated_technical_analysis(test_prices, test_volumes)
        analysis_time = time.time() - start_time
        
        print(f"âœ… æŠ€æœ¯åˆ†æå®Œæˆï¼Œè€—æ—¶: {analysis_time:.4f}s")
        print(f"   è®¡ç®—æŒ‡æ ‡æ•°é‡: {len(analysis_result)}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
        for key, value in list(analysis_result.items())[:5]:
            print(f"   {key}: {value}")
        
        # å†…å­˜ä¼˜åŒ–
        print(f"\nğŸ§¹ å†…å­˜ä¼˜åŒ–...")
        gpu_compute.optimize_memory_usage()
        
        print("\nâœ… GPUåŠ é€Ÿè®¡ç®—æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
