#!/usr/bin/env python3
"""
æ·±åº¦å­¦ä¹ æ¨¡å‹é›†æˆæ¨¡å—
é›†æˆTransformerã€GRUã€CNN-LSTMç­‰å…ˆè¿›æ¨¡å‹
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from typing import Dict, List, Optional, Tuple
import logging
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)


class TransformerPredictor(nn.Module):
    """Transformeré¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_dim=5, d_model=64, nhead=8, num_layers=3, dropout=0.1):
        super(TransformerPredictor, self).__init__()
        self.input_dim = input_dim
        self.d_model = d_model
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, dropout)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1)
        )
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        x = self.input_projection(x)
        x = self.pos_encoding(x)
        x = self.transformer(x)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        x = x[:, -1, :]
        output = self.output_layer(x)
        return output


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:x.size(1), :].transpose(0, 1)
        return self.dropout(x)


class GRUPredictor(nn.Module):
    """GRUé¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_dim=5, hidden_dim=64, num_layers=2, dropout=0.1):
        super(GRUPredictor, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.gru = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        gru_out, _ = self.gru(x)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = gru_out[:, -1, :]
        output = self.output_layer(last_output)
        return output


class CNNLSTMPredictor(nn.Module):
    """CNN-LSTMé¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_dim=5, cnn_filters=32, lstm_hidden=64, dropout=0.1):
        super(CNNLSTMPredictor, self).__init__()
        
        # CNNå±‚
        self.conv1 = nn.Conv1d(input_dim, cnn_filters, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(cnn_filters, cnn_filters * 2, kernel_size=3, padding=1)
        self.pool = nn.MaxPool1d(2)
        self.dropout_cnn = nn.Dropout(dropout)
        
        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=cnn_filters * 2,
            hidden_size=lstm_hidden,
            num_layers=2,
            dropout=dropout,
            batch_first=True
        )
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(lstm_hidden, lstm_hidden // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(lstm_hidden // 2, 1)
        )
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        # è½¬æ¢ä¸ºCNNè¾“å…¥æ ¼å¼: (batch_size, input_dim, seq_len)
        x = x.transpose(1, 2)
        
        # CNNç‰¹å¾æå–
        x = torch.relu(self.conv1(x))
        x = self.dropout_cnn(x)
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        x = self.dropout_cnn(x)
        
        # è½¬æ¢å›LSTMè¾“å…¥æ ¼å¼: (batch_size, seq_len, features)
        x = x.transpose(1, 2)
        
        # LSTMå¤„ç†
        lstm_out, _ = self.lstm(x)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]
        output = self.output_layer(last_output)
        return output


class AutoEncoder(nn.Module):
    """è‡ªç¼–ç å™¨å¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
    
    def __init__(self, input_dim=5, encoding_dim=16):
        super(AutoEncoder, self).__init__()
        
        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, encoding_dim * 2),
            nn.ReLU(),
            nn.Linear(encoding_dim * 2, encoding_dim),
            nn.ReLU()
        )
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, encoding_dim * 2),
            nn.ReLU(),
            nn.Linear(encoding_dim * 2, input_dim)
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


class DeepLearningEnsemble:
    """æ·±åº¦å­¦ä¹ æ¨¡å‹é›†æˆ"""
    
    def __init__(self, device=None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.models = {}
        self.scalers = {}
        self.is_trained = False
        self.sequence_length = 20
        
        print(f"[æ·±åº¦å­¦ä¹ ] æ¨¡å‹é›†æˆåˆå§‹åŒ–ï¼Œè®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        try:
            # Transformeræ¨¡å‹
            self.models['transformer'] = TransformerPredictor().to(self.device)
            
            # GRUæ¨¡å‹
            self.models['gru'] = GRUPredictor().to(self.device)
            
            # CNN-LSTMæ¨¡å‹
            self.models['cnn_lstm'] = CNNLSTMPredictor().to(self.device)
            
            # è‡ªç¼–ç å™¨
            self.models['autoencoder'] = AutoEncoder().to(self.device)
            
            # æ•°æ®ç¼©æ”¾å™¨
            self.scalers['price'] = MinMaxScaler()
            self.scalers['features'] = MinMaxScaler()
            
            print(f"[æ·±åº¦å­¦ä¹ ] åˆå§‹åŒ– {len(self.models)} ä¸ªæ¨¡å‹")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä½¿ç”¨CPUä½œä¸ºå¤‡é€‰
            self.device = 'cpu'
            self._initialize_models()
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        try:
            if len(df) < self.sequence_length + 1:
                raise ValueError(f"æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.sequence_length + 1} ä¸ªæ•°æ®ç‚¹")
            
            # ç‰¹å¾å·¥ç¨‹
            features = self._extract_features(df)
            
            # æ•°æ®ç¼©æ”¾
            features_scaled = self.scalers['features'].fit_transform(features)
            prices = df['price'].values.reshape(-1, 1)
            prices_scaled = self.scalers['price'].fit_transform(prices)
            
            # åˆ›å»ºåºåˆ—æ•°æ®
            X, y = self._create_sequences(features_scaled, prices_scaled.flatten())
            
            return X, y
            
        except Exception as e:
            logger.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return np.array([]), np.array([])
    
    def _extract_features(self, df: pd.DataFrame) -> np.ndarray:
        """æå–ç‰¹å¾"""
        features = []
        
        # ä»·æ ¼ç‰¹å¾
        features.append(df['price'].values)
        features.append(df['bid'].values)
        features.append(df['ask'].values)
        
        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        prices = df['price'].values
        
        # ç§»åŠ¨å¹³å‡
        if len(prices) >= 5:
            ma5 = pd.Series(prices).rolling(5).mean().fillna(prices[0])
            features.append(ma5.values)
        else:
            features.append(prices)
        
        # ä»·æ ¼å˜åŒ–ç‡
        if len(prices) >= 2:
            returns = pd.Series(prices).pct_change().fillna(0)
            features.append(returns.values)
        else:
            features.append(np.zeros_like(prices))
        
        return np.column_stack(features)
    
    def _create_sequences(self, features: np.ndarray, targets: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """åˆ›å»ºåºåˆ—æ•°æ®"""
        X, y = [], []
        
        for i in range(len(features) - self.sequence_length):
            X.append(features[i:i + self.sequence_length])
            y.append(targets[i + self.sequence_length])
        
        return np.array(X), np.array(y)
    
    def train_models(self, df: pd.DataFrame, epochs=50, batch_size=32) -> Dict:
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        try:
            print(f"[è®­ç»ƒ] å¼€å§‹è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
            
            # å‡†å¤‡æ•°æ®
            X, y = self.prepare_data(df)
            if len(X) == 0:
                return {'success': False, 'message': 'æ•°æ®å‡†å¤‡å¤±è´¥'}
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            X_tensor = torch.FloatTensor(X).to(self.device)
            y_tensor = torch.FloatTensor(y).to(self.device)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataset = TensorDataset(X_tensor, y_tensor)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            
            # è®­ç»ƒç»“æœ
            training_results = {}
            
            # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
            for model_name, model in self.models.items():
                if model_name == 'autoencoder':
                    continue  # è‡ªç¼–ç å™¨å•ç‹¬è®­ç»ƒ
                
                print(f"   è®­ç»ƒ {model_name}...")
                result = self._train_single_model(model, dataloader, epochs)
                training_results[model_name] = result
            
            # è®­ç»ƒè‡ªç¼–ç å™¨
            print(f"   è®­ç»ƒ autoencoder...")
            ae_result = self._train_autoencoder(X_tensor, epochs)
            training_results['autoencoder'] = ae_result
            
            self.is_trained = True
            
            print(f"[è®­ç»ƒ] æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå®Œæˆ")
            return {'success': True, 'results': training_results}
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {'success': False, 'message': str(e)}
    
    def _train_single_model(self, model: nn.Module, dataloader: DataLoader, epochs: int) -> Dict:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        try:
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.MSELoss()
            
            model.train()
            losses = []
            
            for epoch in range(epochs):
                epoch_loss = 0
                for batch_X, batch_y in dataloader:
                    optimizer.zero_grad()
                    
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                
                avg_loss = epoch_loss / len(dataloader)
                losses.append(avg_loss)
                
                if epoch % 10 == 0:
                    print(f"     Epoch {epoch}/{epochs}, Loss: {avg_loss:.6f}")
            
            return {'final_loss': losses[-1], 'losses': losses}
            
        except Exception as e:
            logger.error(f"å•æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {'final_loss': float('inf'), 'losses': []}
    
    def _train_autoencoder(self, X_tensor: torch.Tensor, epochs: int) -> Dict:
        """è®­ç»ƒè‡ªç¼–ç å™¨"""
        try:
            model = self.models['autoencoder']
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.MSELoss()
            
            model.train()
            losses = []
            
            # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„æ•°æ®è®­ç»ƒè‡ªç¼–ç å™¨
            X_last = X_tensor[:, -1, :]
            
            for epoch in range(epochs):
                optimizer.zero_grad()
                
                reconstructed = model(X_last)
                loss = criterion(reconstructed, X_last)
                
                loss.backward()
                optimizer.step()
                
                losses.append(loss.item())
                
                if epoch % 10 == 0:
                    print(f"     Epoch {epoch}/{epochs}, Loss: {loss.item():.6f}")
            
            return {'final_loss': losses[-1], 'losses': losses}
            
        except Exception as e:
            logger.error(f"è‡ªç¼–ç å™¨è®­ç»ƒå¤±è´¥: {e}")
            return {'final_loss': float('inf'), 'losses': []}
    
    def predict(self, df: pd.DataFrame) -> Dict:
        """ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        try:
            if not self.is_trained:
                return {'success': False, 'message': 'æ¨¡å‹æœªè®­ç»ƒ'}
            
            if len(df) < self.sequence_length:
                return {'success': False, 'message': f'æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.sequence_length} ä¸ªæ•°æ®ç‚¹'}
            
            # å‡†å¤‡é¢„æµ‹æ•°æ®
            features = self._extract_features(df)
            features_scaled = self.scalers['features'].transform(features)
            
            # è·å–æœ€åä¸€ä¸ªåºåˆ—
            X_pred = features_scaled[-self.sequence_length:].reshape(1, self.sequence_length, -1)
            X_tensor = torch.FloatTensor(X_pred).to(self.device)
            
            # è·å–å„æ¨¡å‹é¢„æµ‹
            predictions = {}
            
            for model_name, model in self.models.items():
                if model_name == 'autoencoder':
                    continue
                
                model.eval()
                with torch.no_grad():
                    pred = model(X_tensor)
                    pred_scaled = pred.cpu().numpy()[0, 0]
                    
                    # åç¼©æ”¾
                    pred_original = self.scalers['price'].inverse_transform([[pred_scaled]])[0, 0]
                    predictions[model_name] = pred_original
            
            # å¼‚å¸¸æ£€æµ‹
            anomaly_score = self._detect_anomaly(X_tensor[:, -1, :])
            
            # é›†æˆé¢„æµ‹
            ensemble_pred = self._ensemble_predictions(predictions)
            
            return {
                'success': True,
                'ensemble_prediction': ensemble_pred,
                'individual_predictions': predictions,
                'anomaly_score': anomaly_score,
                'confidence': self._calculate_confidence(predictions, anomaly_score)
            }
            
        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            return {'success': False, 'message': str(e)}
    
    def _detect_anomaly(self, X_tensor: torch.Tensor) -> float:
        """å¼‚å¸¸æ£€æµ‹"""
        try:
            model = self.models['autoencoder']
            model.eval()
            
            with torch.no_grad():
                reconstructed = model(X_tensor)
                mse = nn.MSELoss()(reconstructed, X_tensor)
                
                # å½’ä¸€åŒ–å¼‚å¸¸åˆ†æ•°
                anomaly_score = min(mse.item() * 100, 1.0)
                return anomaly_score
                
        except Exception as e:
            logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return 0.5
    
    def _ensemble_predictions(self, predictions: Dict) -> float:
        """é›†æˆé¢„æµ‹ç»“æœ"""
        if not predictions:
            return 0.0
        
        # ç®€å•å¹³å‡é›†æˆ
        values = list(predictions.values())
        return np.mean(values)
    
    def _calculate_confidence(self, predictions: Dict, anomaly_score: float) -> float:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
        if not predictions:
            return 0.0
        
        # åŸºäºé¢„æµ‹ä¸€è‡´æ€§å’Œå¼‚å¸¸åˆ†æ•°è®¡ç®—ç½®ä¿¡åº¦
        values = list(predictions.values())
        std = np.std(values)
        mean_val = np.mean(values)
        
        # ä¸€è‡´æ€§åˆ†æ•°ï¼ˆæ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜ï¼‰
        consistency = 1.0 / (1.0 + std / abs(mean_val)) if mean_val != 0 else 0.5
        
        # å¼‚å¸¸åˆ†æ•°å½±å“ï¼ˆå¼‚å¸¸åˆ†æ•°è¶Šé«˜ï¼Œç½®ä¿¡åº¦è¶Šä½ï¼‰
        anomaly_factor = 1.0 - anomaly_score
        
        # ç»¼åˆç½®ä¿¡åº¦
        confidence = (consistency * 0.7 + anomaly_factor * 0.3)
        return max(0.1, min(0.95, confidence))
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'device': self.device,
            'models': list(self.models.keys()),
            'is_trained': self.is_trained,
            'sequence_length': self.sequence_length,
            'cuda_available': torch.cuda.is_available()
        }


def main():
    """æµ‹è¯•å‡½æ•°"""
    print("æ·±åº¦å­¦ä¹ æ¨¡å‹é›†æˆæµ‹è¯•")
    print("=" * 40)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_points = 200
    base_price = 3400
    
    # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®
    price_changes = np.random.normal(0, 5, n_points)
    prices = [base_price]
    for change in price_changes:
        prices.append(prices[-1] + change)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame({
        'price': prices[1:],
        'bid': [p - 0.05 for p in prices[1:]],
        'ask': [p + 0.05 for p in prices[1:]],
        'volume': np.random.randint(1000, 5000, n_points)
    })
    
    # æµ‹è¯•æ·±åº¦å­¦ä¹ æ¨¡å‹
    ensemble = DeepLearningEnsemble()
    
    try:
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        info = ensemble.get_model_info()
        print(f"âœ… æ¨¡å‹ä¿¡æ¯:")
        print(f"   è®¾å¤‡: {info['device']}")
        print(f"   æ¨¡å‹æ•°é‡: {len(info['models'])}")
        print(f"   CUDAå¯ç”¨: {info['cuda_available']}")
        
        # è®­ç»ƒæ¨¡å‹
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        train_result = ensemble.train_models(df, epochs=20, batch_size=16)
        
        if train_result['success']:
            print(f"âœ… è®­ç»ƒæˆåŠŸ")
            
            # è¿›è¡Œé¢„æµ‹
            print(f"\nğŸ”® è¿›è¡Œé¢„æµ‹...")
            pred_result = ensemble.predict(df)
            
            if pred_result['success']:
                print(f"âœ… é¢„æµ‹æˆåŠŸ")
                print(f"   é›†æˆé¢„æµ‹: ${pred_result['ensemble_prediction']:.2f}")
                print(f"   ç½®ä¿¡åº¦: {pred_result['confidence']:.2f}")
                print(f"   å¼‚å¸¸åˆ†æ•°: {pred_result['anomaly_score']:.3f}")
                
                print(f"\nğŸ“Š å„æ¨¡å‹é¢„æµ‹:")
                for model_name, pred in pred_result['individual_predictions'].items():
                    print(f"   {model_name}: ${pred:.2f}")
            else:
                print(f"âŒ é¢„æµ‹å¤±è´¥: {pred_result['message']}")
        else:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {train_result['message']}")
        
        print("\nâœ… æ·±åº¦å­¦ä¹ æ¨¡å‹æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
